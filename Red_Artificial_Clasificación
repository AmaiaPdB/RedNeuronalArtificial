# INTRODUCCIÓN:

## Objetivo: que la red distinga diferentes tipos de zapato.
## Si queremos que la red distinga diferentes tipos zapatos, si le enseñamos pocos, a lo mejor coincide que todas las sandalias que le eseñamos son rojas 
## e interpreta que un zapato rojo es una sandalia (el llamado overfitting). Para que esto no ocurra y aprenda como es cada tipo de zapato, habrá que enseñarle 
## muchos zapatos de diferentes colores, tipos, formas...

## Utilizaremos Fashion MNIST de TensorFlow, un dataset de ropa con 70.000 imágenes en escala de grises agrupadas en 10 categorías de 7.000 imágenes.
## El dataset se divide en un set de entrenamiento o train_images con sus etiquetas train_labels (60.000 imágenes) y en un set de test o test_images 
## con sus etiquetas test_labels (10.000 imágenes). Las etiquetas son números por dos razones: mejor compresión por un sistema computacional 
## y evitar sesgos que pueda introducir un idioma.

## Las imágenes son pequeñas, de 28x28 píxeles (matriz de 28x28), lo cual hará que el ordenador las procese más rápido que si habláramos de imágenes médicas.
## La capa de entrada de la red tendrá que tener 28x28 neuronas, una por píxel
## El modelo funcionará porque en la imagen solo tenemos el objeto que nos iteresa, siempre en la misma posición (centrado), las imágenes son del mismo tamaño, etc. 
## Si no, necesitaríamos una red que extrajera los contenidos de la imagen más que analizar la imagen como un todo, es decir, 
## no nos serviría una red neuronal profunda, sino que haría falta una CNN.

# PROGRAMACIÓN:

## 1º: Importar las librerías necesarias y el dataset Fashion MNIST.

import tensorflow as tf

mnist = tf.keras.datasets.fashion_mnist

(training_images, training_labels), (test_images, test_labels) = mnist.load_data()

## Podemos ver los elementos que queramos de las imágenes importadas. 
## En el ejemplo de acontinuación vemos la imagen 0 y su etiqueta.
## Estos sistemas ven las imágenes como arrays, es decir, como una matríz de 28 filas y 28 columnas.

import matplotlib.pyplot as plt

print('La etiqueta de la imagen escogida es: ', training_labels[0])   #Vemos la etiqueta.
print(training_images[0])                                             #Vemos la imagen como una matriz de números.
plt.imshow(training_images[0])                                        #La imagen como la vemos nosotros. 

## 2º: Normalizar las imágenes.

## Vemos que los pixels de la matriz toman valores entre 0 y 255, es decir, pueden tomar 256 grises distintos, lo que significa que la profundidad es de 
## 8 bits (2^8 es 256). Ahora tendremos que NORMALIZAR el dataset. Se ha demostrado que las redes funcionan mejor manejando valores entre 0 y 1, 
## entonces nos interesaría que los pixels tomarán valores entre 0 y 1, para lo cual no tenemos más que dividir por el número más alto que pueden tomar, 
## en este caso 255. Así, el píxel que tomaba el valor 0 seguirá tomando el valor 0, el píxel que tomaba el 255 ahora tomará el 1, el que tomaba el 10 
## ahora 10/255 y así con todos.

training_images  = training_images / 255.0
test_images = test_images / 255.0

## 3º: Diseñar la arquitectura del modelo.

## Es recomendable comenzar el diseño por la capa de entrada y la de salida, dedicando especial atención a cómo son los inputs y outputs que tenemos y 
## necesitamos, respectivamente. 
## En este ejemplo, metemos un tamaño de entrada de 28x28 píxels, por lo que necesitaremos un número de neuronas en la primera capa, la capa de entrada, 
## de 28x28. En cuanto a la capa de salida, si clasificaremos en 10 clases diferentes, necesitaremos que tenga 10 neuronas.

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                    tf.keras.layers.Dense(128, activation='relu'), 
                                    tf.keras.layers.Dense(10, activation='softmax')])
                                    
## 4º: Compilar el modelo. 

## Programamos la función de error o pérdida y la función de optimización o aprendizaje.
## Fijamos la variable en la que nos fijaremos. En este caso, la "accuracy".

model.compile(optimizer = tf.keras.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])

## 5º: Entrenar el modelo.

## Consiste en "ajustar los ejemplos conocidos a sus etiquetas", de ahí que la función de entrenamiento se llame fit. Llamamos a la función fit y le pasamos 
## de argumentos: nuestro dataset de entrenamiento, nuestras etiquetas, y nuestras épocas (el número de veces que revisará el dataset e irá aprendiendo).

model.fit(training_images, training_labels, epochs=20)

## 6º: Testar el modelo con el set de test.

## Calcularemos la ACCURACY con las imágenes de test. 

model.evaluate(test_images, test_labels)

## En nuestro caso, nos devolvió una "accuracy" de 0.8839.

## 7º: Introducir nuevos ejemplos.

## Se pueden hacer nuevas predicciones prediction1, prediction 2... llamando al módulo model.predict e introduciendo como argumento la nueva imagen.

#prediction = model.predict(newimage)



